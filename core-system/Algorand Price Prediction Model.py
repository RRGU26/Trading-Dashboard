#!/usr/bin/env python3
"""
ALGORAND PREDICTION MODEL V2.0 - FIXED VERSION
ðŸš€ Fixes for critical performance issues:
âœ… Fix #1: Directional bias correction (was 0% down accuracy)
âœ… Fix #2: Confidence recalibration (was 94.5% overconfident)  
âœ… Fix #3: Volatility filtering (skip high volatility periods)
"""

import sys
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score, accuracy_score
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from xgboost import XGBRegressor
from lightgbm import LGBMRegressor
import warnings
from datetime import datetime, timedelta
import os
import traceback
import time
import sqlite3
from typing import Dict, List, Tuple, Optional
import joblib

# Setup
warnings.filterwarnings("ignore")
script_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.append(script_dir)

try:
    import data_fetcher
    USE_DATA_FETCHER = True
except ImportError:
    print("ERROR: data_fetcher module not found. Cannot proceed.")
    sys.exit(1)

# Configuration
OUTPUTS_DIR = "outputs"
os.makedirs(OUTPUTS_DIR, exist_ok=True)

ALGO_SYMBOL = "ALGO-USD"
BTC_SYMBOL = "BTC-USD"
ETH_SYMBOL = "ETH-USD"

class DatabaseIntegrator:
    """Built-in database integration for modular report system"""
    
    def __init__(self):
        self.db_path = self._find_database()
        self.model_name = "Algorand Model V2"
        self.symbol = "ALGO-USD"
        
    def _find_database(self) -> str:
        """Find the models dashboard database"""
        possible_paths = [
            os.path.join(os.path.expanduser("~"), "OneDrive", "Desktop", "models_dashboard.db"),
            os.path.join(os.path.expanduser("~"), "Desktop", "models_dashboard.db"),
            "models_dashboard.db",
            os.path.join(".", "models_dashboard.db")
        ]
        
        for path in possible_paths:
            if os.path.exists(path):
                print(f"[OK] Found database: {path}")
                return path
        
        print("[WARNING] Database not found - will run in standalone mode")
        return None
    
    def save_prediction(self, prediction_data: Dict) -> bool:
        """Save prediction to model_predictions table"""
        if not self.db_path:
            return False
            
        try:
            conn = sqlite3.connect(self.db_path)
            cursor = conn.cursor()
            
            current_price = prediction_data['current_price']
            predicted_price = prediction_data['predicted_price']
            predicted_return = prediction_data['predicted_return']
            confidence = prediction_data['confidence']
            horizon_days = prediction_data['horizon_days']
            
            prediction_date = datetime.now().date()
            target_date = prediction_date + timedelta(days=horizon_days)
            
            # IMPROVED ACTION LOGIC - Fix #1: Better directional bias handling
            if predicted_return > 2.0:
                action = "BUY"
            elif predicted_return > 0.5:
                action = "HOLD"
            elif predicted_return > -0.5:
                action = "HOLD"
            elif predicted_return > -2.0:
                action = "SELL"
            else:
                action = "STRONG SELL"
            
            cursor.execute("""
                INSERT INTO model_predictions (
                    model, symbol, prediction_date, target_date, horizon,
                    current_price, predicted_price, actual_price, confidence,
                    suggested_action, error_pct, direction_correct,
                    expected_return, actual_return, return_error
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                self.model_name, self.symbol, prediction_date, target_date, horizon_days,
                current_price, predicted_price, None, confidence,
                action, None, None, predicted_return, None, None
            ))
            
            conn.commit()
            conn.close()
            
            print(f"[OK] Prediction saved to database")
            print(f"   Date: {prediction_date} -> {target_date} ({horizon_days}d)")
            print(f"   Price: ${current_price:.4f} -> ${predicted_price:.4f} ({predicted_return:+.2f}%)")
            print(f"   Action: {action} (Confidence: {confidence:.1f}%)")
            
            return True
            
        except Exception as e:
            print(f"[ERROR] Error saving prediction: {e}")
            return False

def fallback_get_price_data(symbol: str, start_date: str, end_date: str) -> pd.DataFrame:
    """Fallback function for data fetching"""
    try:
        return data_fetcher.get_historical_data(symbol, start_date, end_date)
    except Exception as e:
        print(f"    [WARNING] data_fetcher failed for {symbol}: {e}")
        return None

class AlgorandModelV2Fixed:
    """Fixed version of Algorand prediction model addressing critical issues"""
    
    def __init__(self):
        self.models = {}
        self.scalers = {}
        self.feature_cols = []
        self.prediction_horizons = [1, 3, 5, 7]
        self.best_horizon = 3
        self.market_regime = "UNKNOWN"
        self.performance_metrics = {}
        self.correlation_assets = []
        self.db_integrator = DatabaseIntegrator()
        
        # FIX #3: Volatility filtering thresholds
        self.volatility_threshold = 15.0  # Skip predictions if volatility > 15%
        self.skip_high_volatility = True
        
        print("[FIXES APPLIED]")
        print("Fix #1: Directional bias correction enabled")
        print("Fix #2: Confidence recalibration enabled")
        print("Fix #3: Volatility filtering enabled (threshold: 15%)")
        print()

    def detect_market_conditions(self, df: pd.DataFrame) -> Dict:
        """Enhanced market condition detection for Fix #1 & #3"""
        if len(df) < 10:
            return {"regime": "UNKNOWN", "volatility": 0, "skip_prediction": False}
        
        recent_data = df.tail(10)  # Last 10 days
        
        # Calculate trend
        price_trend = (recent_data['Close'].iloc[-1] / recent_data['Close'].iloc[0] - 1) * 100
        
        # Calculate volatility - Fix #3
        volatility = recent_data['Close'].pct_change().std() * np.sqrt(252) * 100  # Annualized volatility
        
        # Fix #1: Detect bear market conditions for directional bias correction
        ma_5 = recent_data['Close'].rolling(5).mean().iloc[-1]
        ma_10 = df['Close'].rolling(10).mean().iloc[-1]
        current_price = recent_data['Close'].iloc[-1]
        
        bear_signals = 0
        if current_price < ma_5:
            bear_signals += 1
        if current_price < ma_10:
            bear_signals += 1
        if price_trend < -5:  # Declining >5% over 10 days
            bear_signals += 1
        
        is_bear_market = bear_signals >= 2
        
        # Market regime
        if price_trend > 10 and volatility < 20:
            regime = "BULL_LOW_VOL"
        elif price_trend > 5 and volatility < 30:
            regime = "BULL_MODERATE"
        elif price_trend < -10:
            regime = "BEAR" if not is_bear_market else "STRONG_BEAR"
        elif volatility > 40:
            regime = "HIGH_VOLATILITY"
        else:
            regime = "SIDEWAYS"
        
        # Fix #3: Skip prediction if volatility too high
        skip_prediction = self.skip_high_volatility and volatility > self.volatility_threshold
        
        conditions = {
            "regime": regime,
            "volatility": volatility,
            "price_trend": price_trend,
            "is_bear_market": is_bear_market,
            "bear_signals": bear_signals,
            "skip_prediction": skip_prediction,
            "current_price": current_price,
            "ma_5": ma_5,
            "ma_10": ma_10
        }
        
        print(f"[MARKET CONDITIONS]")
        print(f"Regime: {regime}")
        print(f"Volatility: {volatility:.1f}%")
        print(f"Price Trend (10d): {price_trend:.1f}%")
        print(f"Bear Market Signals: {bear_signals}/3")
        if skip_prediction:
            print(f"[WARNING] High volatility ({volatility:.1f}%) - prediction may be skipped")
        
        return conditions

    def apply_directional_bias_correction(self, raw_prediction: float, current_price: float, 
                                        market_conditions: Dict) -> float:
        """Fix #1: Correct directional bias based on market conditions"""
        
        # Calculate raw predicted return
        raw_return = (raw_prediction / current_price - 1) * 100
        
        # Apply bias correction based on market conditions
        if market_conditions["is_bear_market"]:
            # In bear markets, reduce upward bias
            if raw_return > 0:
                # Reduce positive predictions by 30-50% in bear markets
                correction_factor = 0.5 if market_conditions["bear_signals"] >= 3 else 0.7
                corrected_return = raw_return * correction_factor
                print(f"[BIAS CORRECTION] Bear market detected - reducing upward bias")
                print(f"   Raw return: {raw_return:+.2f}% -> Corrected: {corrected_return:+.2f}%")
            else:
                # Keep or slightly amplify negative predictions in bear markets
                corrected_return = raw_return * 1.1
        else:
            # In normal/bull markets, slight downward adjustment to reduce overconfidence
            if raw_return > 5:
                corrected_return = raw_return * 0.85  # Reduce extreme positive predictions
            elif raw_return < -5:
                corrected_return = raw_return * 0.85  # Reduce extreme negative predictions  
            else:
                corrected_return = raw_return
        
        # Convert back to price
        corrected_prediction = current_price * (1 + corrected_return / 100)
        
        return corrected_prediction

    def calibrate_confidence(self, raw_confidence: float, market_conditions: Dict,
                           model_agreement: float) -> float:
        """Fix #2: Recalibrate confidence to match historical accuracy"""
        
        # Base recalibration: Scale down from 94.5% average to realistic 40-60% range
        # Historical accuracy is 33.3%, so max confidence should be around 50-60%
        base_calibrated = raw_confidence * 0.6  # Scale down significantly
        
        # Adjust for market conditions
        if market_conditions["volatility"] > 25:
            # High volatility = lower confidence
            volatility_penalty = min(20, (market_conditions["volatility"] - 25) * 0.5)
            base_calibrated -= volatility_penalty
        
        if market_conditions["is_bear_market"]:
            # Bear markets are harder to predict
            base_calibrated -= 10
        
        # Adjust for model agreement
        if model_agreement > 0.02:  # High disagreement between models
            base_calibrated -= 15
        elif model_agreement < 0.005:  # High agreement
            base_calibrated += 5
        
        # Ensure confidence is in reasonable range
        final_confidence = max(20, min(75, base_calibrated))  # Cap at 75% max
        
        print(f"[CONFIDENCE CALIBRATION]")
        print(f"Raw confidence: {raw_confidence:.1f}%")
        print(f"Calibrated confidence: {final_confidence:.1f}%")
        if final_confidence < raw_confidence * 0.8:
            print(f"   Applied significant downward adjustment for realism")
        
        return final_confidence

    def fetch_algo_data(self) -> pd.DataFrame:
        """Simplified data fetching focused on ALGO"""
        print("[DATA FETCHING] Fetching ALGO data...")
        
        end_date = datetime.now().strftime("%Y-%m-%d")
        start_date = (datetime.now() - timedelta(days=365)).strftime("%Y-%m-%d")  # 1 year
        
        for symbol in [ALGO_SYMBOL, 'ALGO-USD', 'ALGO']:
            try:
                df = fallback_get_price_data(symbol, start_date, end_date)
                if df is not None and not df.empty and len(df) > 50:
                    print(f"   [OK] ALGO data: {len(df)} rows ({symbol})")
                    return self._prepare_dataframe(df)
            except Exception as e:
                print(f"   [ERROR] Failed to fetch {symbol}: {e}")
        
        print("[ERROR] Could not fetch ALGO data!")
        return None

    def _prepare_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """Prepare and clean dataframe with basic features"""
        # Ensure numeric columns
        numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']
        for col in numeric_cols:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # Basic features for the fixed model
        df['returns_1d'] = df['Close'].pct_change()
        df['returns_3d'] = df['Close'].pct_change(3)
        df['returns_5d'] = df['Close'].pct_change(5)
        
        # Volatility features
        df['volatility_5d'] = df['returns_1d'].rolling(5).std()
        df['volatility_10d'] = df['returns_1d'].rolling(10).std()
        
        # Moving averages
        df['ma_5'] = df['Close'].rolling(5).mean()
        df['ma_10'] = df['Close'].rolling(10).mean()
        df['ma_20'] = df['Close'].rolling(20).mean()
        
        # Momentum indicators
        df['momentum_3d'] = df['Close'] / df['Close'].shift(3) - 1
        df['momentum_5d'] = df['Close'] / df['Close'].shift(5) - 1
        
        # RSI
        delta = df['Close'].diff()
        gain = delta.where(delta > 0, 0).rolling(14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()
        rs = gain / (loss + 1e-8)
        df['rsi'] = 100 - (100 / (1 + rs))
        
        return df

    def create_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """Create focused feature set for the fixed model"""
        print("[FEATURE ENGINEERING] Creating focused feature set...")
        
        feature_df = df.copy()
        
        # Trend features
        feature_df['trend_5d'] = (feature_df['Close'] > feature_df['ma_5']).astype(int)
        feature_df['trend_10d'] = (feature_df['Close'] > feature_df['ma_10']).astype(int)
        feature_df['trend_20d'] = (feature_df['Close'] > feature_df['ma_20']).astype(int)
        
        # Price position features
        feature_df['price_pos_5d'] = (feature_df['Close'] - feature_df['ma_5']) / feature_df['ma_5']
        feature_df['price_pos_10d'] = (feature_df['Close'] - feature_df['ma_10']) / feature_df['ma_10'] 
        
        # Volatility features
        feature_df['vol_rank'] = feature_df['volatility_5d'].rolling(30).rank(pct=True)
        
        # Momentum crossovers
        feature_df['momentum_accel'] = feature_df['momentum_3d'] - feature_df['momentum_3d'].shift(1)
        
        # Select final features
        feature_cols = [
            'returns_1d', 'returns_3d', 'returns_5d',
            'volatility_5d', 'volatility_10d', 'vol_rank',
            'momentum_3d', 'momentum_5d', 'momentum_accel',
            'rsi', 'trend_5d', 'trend_10d', 'trend_20d',
            'price_pos_5d', 'price_pos_10d'
        ]
        
        # Clean data
        for col in feature_cols:
            if col in feature_df.columns:
                feature_df[col] = feature_df[col].replace([np.inf, -np.inf], np.nan)
                feature_df[col] = feature_df[col].fillna(method='ffill').fillna(0)
        
        print(f"   [OK] Created {len(feature_cols)} focused features")
        return feature_df[feature_cols + ['Close']]

    def train_simple_ensemble(self, df: pd.DataFrame, horizon: int = 3) -> Dict:
        """Train simplified ensemble model"""
        print(f"[MODEL TRAINING] Training ensemble for {horizon}-day horizon...")
        
        # Prepare training data
        feature_df = self.create_features(df)
        feature_cols = [col for col in feature_df.columns if col != 'Close']
        
        # Create target
        feature_df[f'target_{horizon}d'] = feature_df['Close'].shift(-horizon)
        feature_df = feature_df.dropna()
        
        if len(feature_df) < 100:
            raise ValueError(f"Insufficient data: {len(feature_df)} samples")
        
        # Split data
        split_idx = int(len(feature_df) * 0.8)
        X_train = feature_df[feature_cols].iloc[:split_idx]
        X_test = feature_df[feature_cols].iloc[split_idx:]
        y_train = feature_df[f'target_{horizon}d'].iloc[:split_idx]
        y_test = feature_df[f'target_{horizon}d'].iloc[split_idx:]
        
        # Scale features
        scaler = RobustScaler()
        X_train_scaled = pd.DataFrame(
            scaler.fit_transform(X_train),
            index=X_train.index, columns=X_train.columns
        )
        X_test_scaled = pd.DataFrame(
            scaler.transform(X_test),
            index=X_test.index, columns=X_test.columns
        )
        
        # Train models
        models = {
            'xgboost': XGBRegressor(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=42),
            'lightgbm': LGBMRegressor(n_estimators=100, max_depth=4, learning_rate=0.1, random_state=42, verbose=-1),
            'random_forest': RandomForestRegressor(n_estimators=50, max_depth=8, random_state=42)
        }
        
        trained_models = {}
        predictions = {}
        
        for name, model in models.items():
            try:
                model.fit(X_train_scaled, y_train)
                pred = model.predict(X_test_scaled)
                
                r2 = r2_score(y_test, pred)
                if r2 > -0.5:  # Only keep reasonably performing models
                    trained_models[name] = model
                    predictions[name] = pred
                    print(f"   {name}: RÂ² = {r2:.3f}")
                else:
                    print(f"   {name}: RÂ² = {r2:.3f} (excluded)")
            except Exception as e:
                print(f"   [ERROR] {name} failed: {e}")
        
        if not trained_models:
            raise ValueError("No models trained successfully")
        
        # Ensemble prediction
        ensemble_pred = np.mean(list(predictions.values()), axis=0)
        ensemble_r2 = r2_score(y_test, ensemble_pred)
        
        print(f"   [ENSEMBLE] RÂ² = {ensemble_r2:.3f}")
        
        return {
            'models': trained_models,
            'scaler': scaler,
            'feature_cols': feature_cols,
            'ensemble_r2': ensemble_r2,
            'horizon': horizon
        }

    def make_prediction(self, df: pd.DataFrame, current_price: float, horizon: int = 3) -> Dict:
        """Make prediction with all three fixes applied"""
        print(f"[PREDICTION] Making {horizon}-day prediction...")
        
        # Check market conditions first (Fix #3)
        market_conditions = self.detect_market_conditions(df)
        
        if market_conditions["skip_prediction"]:
            print(f"[SKIP] High volatility ({market_conditions['volatility']:.1f}%) - skipping prediction")
            return {
                'skipped': True,
                'reason': f"High volatility ({market_conditions['volatility']:.1f}%)",
                'current_price': current_price
            }
        
        # Train model
        training_results = self.train_simple_ensemble(df, horizon)
        
        # Get latest features
        feature_df = self.create_features(df)
        latest_features = feature_df[training_results['feature_cols']].iloc[-1:].fillna(0)
        
        # Scale and predict
        scaler = training_results['scaler']
        latest_scaled = scaler.transform(latest_features)
        
        # Get individual predictions
        individual_preds = {}
        for name, model in training_results['models'].items():
            pred = model.predict(latest_scaled)[0]
            individual_preds[name] = pred
        
        # Raw ensemble prediction
        raw_prediction = np.mean(list(individual_preds.values()))
        model_agreement = np.std(list(individual_preds.values()))
        
        # Apply Fix #1: Directional bias correction
        corrected_prediction = self.apply_directional_bias_correction(
            raw_prediction, current_price, market_conditions
        )
        
        # Calculate raw confidence
        raw_confidence = max(20, min(95, 100 - (model_agreement / current_price * 100)))
        
        # Apply Fix #2: Confidence calibration
        calibrated_confidence = self.calibrate_confidence(
            raw_confidence, market_conditions, model_agreement
        )
        
        # Final results
        predicted_return = (corrected_prediction / current_price - 1) * 100
        
        prediction_result = {
            'current_price': current_price,
            'predicted_price': corrected_prediction,
            'predicted_return': predicted_return,
            'confidence': calibrated_confidence,
            'horizon_days': horizon,
            'market_regime': market_conditions['regime'],
            'individual_predictions': individual_preds,
            'model_agreement': model_agreement,
            'fixes_applied': {
                'directional_bias_correction': True,
                'confidence_recalibration': True,
                'volatility_filtering': True
            },
            'market_conditions': market_conditions
        }
        
        print(f"[RESULTS]")
        print(f"Current: ${current_price:.4f}")
        print(f"Predicted: ${corrected_prediction:.4f} ({predicted_return:+.2f}%)")
        print(f"Confidence: {calibrated_confidence:.1f}% (calibrated from {raw_confidence:.1f}%)")
        print(f"Market Regime: {market_conditions['regime']}")
        
        # Save to database
        self.db_integrator.save_prediction(prediction_result)
        
        return prediction_result

def main():
    """Main execution with fixes applied"""
    print("=" * 70)
    print("ALGORAND PREDICTION MODEL V2.0 - FIXED VERSION")
    print("=" * 70)
    print("Fix #1: Directional bias correction (was 0% down accuracy)")
    print("Fix #2: Confidence recalibration (was 94.5% overconfident)")
    print("Fix #3: Volatility filtering (skip high volatility periods)")
    print("=" * 70)
    
    try:
        # Initialize fixed model
        model = AlgorandModelV2Fixed()
        
        # Fetch data
        algo_df = model.fetch_algo_data()
        if algo_df is None:
            print("[ERROR] Failed to fetch ALGO data")
            return
        
        # Get current price
        current_price = data_fetcher.get_current_price(ALGO_SYMBOL)
        if current_price is None:
            current_price = algo_df['Close'].iloc[-1]
            print(f"[WARNING] Using last available price: ${current_price:.4f}")
        
        # Make prediction with fixes
        prediction = model.make_prediction(algo_df, current_price)
        
        if prediction.get('skipped'):
            print(f"\n[SKIPPED] {prediction['reason']}")
            print("This is Fix #3 in action - avoiding predictions during high volatility")
            return
        
        # Generate summary report
        print("\n" + "=" * 70)
        print("PREDICTION SUMMARY WITH FIXES")
        print("=" * 70)
        print(f"Current Price: ${prediction['current_price']:.4f}")
        print(f"Predicted Price: ${prediction['predicted_price']:.4f}")
        print(f"Expected Return: {prediction['predicted_return']:+.2f}%")
        print(f"Confidence: {prediction['confidence']:.1f}%")
        print(f"Market Regime: {prediction['market_regime']}")
        print(f"Horizon: {prediction['horizon_days']} days")
        
        print(f"\nFIXES APPLIED:")
        print(f"[OK] Directional bias correction")
        print(f"[OK] Confidence recalibration")
        print(f"[OK] Volatility filtering")
        
        print(f"\n[SUCCESS] Prediction saved to database")
        print("=" * 70)
        
    except Exception as e:
        print(f"[ERROR] Main execution failed: {e}")
        traceback.print_exc()

if __name__ == "__main__":
    main()